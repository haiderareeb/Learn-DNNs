{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the problem that Glorot initialization and He initialization aim\n",
    "to fix?\n",
    "> Glorot initialization and He initialization were designed to make the output standard deviation as close as possible to the input standard deviation, at least at the beginning of training. This reduces the vanishing/exploding gradients problem.\n",
    "\n",
    "2. Is it OK to initialize all the weights to the same value as long as that\n",
    "value is selected randomly using He initialization?\n",
    "> No, all weights should be sampled independently; they should not all have the same initial value. One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent), and backpropagation will be unable to break it. Concretely, this means that all the neurons in any given layer will always have the same weights. It's like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution.\n",
    "\n",
    "\n",
    "3. Is it OK to initialize the bias terms to 0?\n",
    "> It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights, and that's OK too; it does not make much difference.\n",
    "\n",
    "4. In which cases would you want to use each of the activation functions\n",
    "we discussed in this chapter?\n",
    "\n",
    "* ReLU is usually a good default for the hidden layers, as it is fast and yields good results. Its ability to output precisely zero can also be useful in some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementations as well as from hardware acceleration.\n",
    "\n",
    "* The leaky ReLU variants of ReLU can improve the model's quality without hindering its speed too much compared to ReLU. For large neural nets and more complex problems, GLU, Swish and Mish can give you a slightly higher quality model, but they have a computational cost.\n",
    "\n",
    "* The hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number in a fixed range (by default between –1 and 1), but nowadays it is not used much in hidden layers, except in recurrent nets.\n",
    "\n",
    "* The sigmoid activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification), but it is rarely used in hidden layers (there are exceptions—for example, for the coding layer of variational autoencoders; see Chapter 17).\n",
    "\n",
    "* The softplus activation function is useful in the output layer when you need to ensure that the output will always be positive. \n",
    "\n",
    "* The softmax activation function is useful in the output layer to estimate probabilities for mutually exclusive classes, but it is rarely (if ever) used in hidden layers.\n",
    "\n",
    "5. What may happen if you set the momentum hyperparameter too close to\n",
    "1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "> If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\n",
    "\n",
    "\n",
    "6. Name three ways you can produce a sparse model.\n",
    ">One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights. For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity. A third option is to use the TensorFlow Model Optimization Toolkit.\n",
    "\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e.,\n",
    "making predictions on new instances)? What about MC dropout?\n",
    "> Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor of 10 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "> a. Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the Swish activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haide\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,activation=\"swish\",\n",
    "                                    kernel_initializer=\"he_normal\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with tf.keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding output layer\n",
    "model.add(tf.keras.layers.Dense(10,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate = 5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading CIFAR10 dataset\n",
    "cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "(X_train_full,y_train_full),(X_test,y_test) = cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the callbacks we need and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_model.keras\",\n",
    "                                                         save_best_only=True)\n",
    "from pathlib import Path\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb,model_checkpoint_cb,tensorboard_cb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eba56f39890c597f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eba56f39890c597f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_cifar10_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.1392 - loss: 24.0809 - val_accuracy: 0.1984 - val_loss: 2.3205\n",
      "Epoch 2/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.2153 - loss: 2.2372 - val_accuracy: 0.2568 - val_loss: 2.0980\n",
      "Epoch 3/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.2529 - loss: 2.0582 - val_accuracy: 0.2874 - val_loss: 1.9832\n",
      "Epoch 4/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.2887 - loss: 1.9595 - val_accuracy: 0.3052 - val_loss: 1.9169\n",
      "Epoch 5/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3157 - loss: 1.8911 - val_accuracy: 0.3290 - val_loss: 1.8332\n",
      "Epoch 6/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.3396 - loss: 1.8290 - val_accuracy: 0.3500 - val_loss: 1.8082\n",
      "Epoch 7/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3555 - loss: 1.7758 - val_accuracy: 0.3504 - val_loss: 1.7881\n",
      "Epoch 8/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3744 - loss: 1.7303 - val_accuracy: 0.3816 - val_loss: 1.7244\n",
      "Epoch 9/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.3921 - loss: 1.6881 - val_accuracy: 0.3912 - val_loss: 1.7026\n",
      "Epoch 10/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.3997 - loss: 1.6607 - val_accuracy: 0.3894 - val_loss: 1.6927\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4106 - loss: 1.6367 - val_accuracy: 0.3906 - val_loss: 1.6932\n",
      "Epoch 12/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4182 - loss: 1.6109 - val_accuracy: 0.4092 - val_loss: 1.6671\n",
      "Epoch 13/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.4282 - loss: 1.5878 - val_accuracy: 0.4134 - val_loss: 1.6411\n",
      "Epoch 14/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.4367 - loss: 1.5690 - val_accuracy: 0.4130 - val_loss: 1.6332\n",
      "Epoch 15/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4439 - loss: 1.5515 - val_accuracy: 0.4156 - val_loss: 1.6209\n",
      "Epoch 16/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - accuracy: 0.4493 - loss: 1.5320 - val_accuracy: 0.4306 - val_loss: 1.5963\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4556 - loss: 1.5155 - val_accuracy: 0.4272 - val_loss: 1.6078\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4629 - loss: 1.5029 - val_accuracy: 0.4236 - val_loss: 1.6117\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4675 - loss: 1.4866 - val_accuracy: 0.4318 - val_loss: 1.5827\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4712 - loss: 1.4707 - val_accuracy: 0.4360 - val_loss: 1.5791\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4735 - loss: 1.4607 - val_accuracy: 0.4372 - val_loss: 1.5908\n",
      "Epoch 22/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4785 - loss: 1.4497 - val_accuracy: 0.4364 - val_loss: 1.5860\n",
      "Epoch 23/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4820 - loss: 1.4358 - val_accuracy: 0.4424 - val_loss: 1.5748\n",
      "Epoch 24/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4884 - loss: 1.4255 - val_accuracy: 0.4366 - val_loss: 1.5821\n",
      "Epoch 25/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4908 - loss: 1.4133 - val_accuracy: 0.4286 - val_loss: 1.5869\n",
      "Epoch 26/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.4954 - loss: 1.4018 - val_accuracy: 0.4368 - val_loss: 1.5865\n",
      "Epoch 27/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.4989 - loss: 1.3948 - val_accuracy: 0.4414 - val_loss: 1.5857\n",
      "Epoch 28/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5034 - loss: 1.3814 - val_accuracy: 0.4350 - val_loss: 1.5870\n",
      "Epoch 29/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5064 - loss: 1.3740 - val_accuracy: 0.4336 - val_loss: 1.5910\n",
      "Epoch 30/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5081 - loss: 1.3623 - val_accuracy: 0.4374 - val_loss: 1.5825\n",
      "Epoch 31/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5140 - loss: 1.3527 - val_accuracy: 0.4370 - val_loss: 1.5878\n",
      "Epoch 32/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5177 - loss: 1.3438 - val_accuracy: 0.4354 - val_loss: 1.5990\n",
      "Epoch 33/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5209 - loss: 1.3344 - val_accuracy: 0.4348 - val_loss: 1.5930\n",
      "Epoch 34/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.5225 - loss: 1.3270 - val_accuracy: 0.4310 - val_loss: 1.6111\n",
      "Epoch 35/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5256 - loss: 1.3199 - val_accuracy: 0.4394 - val_loss: 1.6020\n",
      "Epoch 36/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5313 - loss: 1.3082 - val_accuracy: 0.4334 - val_loss: 1.6149\n",
      "Epoch 37/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5335 - loss: 1.3024 - val_accuracy: 0.4360 - val_loss: 1.5971\n",
      "Epoch 38/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5344 - loss: 1.2977 - val_accuracy: 0.4440 - val_loss: 1.6039\n",
      "Epoch 39/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5391 - loss: 1.2849 - val_accuracy: 0.4440 - val_loss: 1.5886\n",
      "Epoch 40/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5399 - loss: 1.2767 - val_accuracy: 0.4406 - val_loss: 1.5994\n",
      "Epoch 41/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - accuracy: 0.5440 - loss: 1.2703 - val_accuracy: 0.4472 - val_loss: 1.5866\n",
      "Epoch 42/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5476 - loss: 1.2644 - val_accuracy: 0.4436 - val_loss: 1.5908\n",
      "Epoch 43/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5500 - loss: 1.2557 - val_accuracy: 0.4410 - val_loss: 1.6181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19c839b2390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,epochs=100,\n",
    "          validation_data=(X_valid,y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4481 - loss: 1.5651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.574840784072876, 0.4424000084400177]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c.\n",
    "*Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.1864 - loss: 2.2226 - val_accuracy: 0.3152 - val_loss: 1.9800\n",
      "Epoch 2/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.3415 - loss: 1.8162 - val_accuracy: 0.3678 - val_loss: 1.7777\n",
      "Epoch 3/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.3926 - loss: 1.6878 - val_accuracy: 0.3856 - val_loss: 1.6994\n",
      "Epoch 4/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 17ms/step - accuracy: 0.4319 - loss: 1.5968 - val_accuracy: 0.3880 - val_loss: 1.6845\n",
      "Epoch 5/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.4579 - loss: 1.5241 - val_accuracy: 0.3822 - val_loss: 1.7270\n",
      "Epoch 6/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.4837 - loss: 1.4566 - val_accuracy: 0.3894 - val_loss: 1.7192\n",
      "Epoch 7/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.5025 - loss: 1.3945 - val_accuracy: 0.3976 - val_loss: 1.6906\n",
      "Epoch 8/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.5285 - loss: 1.3388 - val_accuracy: 0.3922 - val_loss: 1.7276\n",
      "Epoch 9/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.5517 - loss: 1.2763 - val_accuracy: 0.3972 - val_loss: 1.7508\n",
      "Epoch 10/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.5754 - loss: 1.2233 - val_accuracy: 0.4070 - val_loss: 1.7455\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.5894 - loss: 1.1793 - val_accuracy: 0.3758 - val_loss: 1.9785\n",
      "Epoch 12/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.6058 - loss: 1.1335 - val_accuracy: 0.4106 - val_loss: 1.8490\n",
      "Epoch 13/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.6255 - loss: 1.0908 - val_accuracy: 0.3894 - val_loss: 1.9631\n",
      "Epoch 14/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.6293 - loss: 1.0649 - val_accuracy: 0.3762 - val_loss: 2.0092\n",
      "Epoch 15/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.6439 - loss: 1.0249 - val_accuracy: 0.4152 - val_loss: 1.8871\n",
      "Epoch 16/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.6544 - loss: 0.9886 - val_accuracy: 0.4102 - val_loss: 1.9687\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 16ms/step - accuracy: 0.6625 - loss: 0.9659 - val_accuracy: 0.4024 - val_loss: 1.9895\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.6778 - loss: 0.9295 - val_accuracy: 0.3944 - val_loss: 2.1522\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.6863 - loss: 0.9031 - val_accuracy: 0.3992 - val_loss: 2.1219\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 17ms/step - accuracy: 0.6972 - loss: 0.8690 - val_accuracy: 0.4128 - val_loss: 2.1428\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.7008 - loss: 0.8530 - val_accuracy: 0.3922 - val_loss: 2.1921\n",
      "Epoch 22/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.7150 - loss: 0.8152 - val_accuracy: 0.3920 - val_loss: 2.1855\n",
      "Epoch 23/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.7185 - loss: 0.8035 - val_accuracy: 0.3964 - val_loss: 2.3392\n",
      "Epoch 24/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.7338 - loss: 0.7678 - val_accuracy: 0.4076 - val_loss: 2.3241\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3927 - loss: 1.6703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6845201253890991, 0.3880000114440918]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(\"swish\"))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,\n",
    "                                                     restore_best_weights=True)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.keras\",\n",
    "                                                         save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_bn_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Is the model converging faster than before?* Much faster! The previous model took 43 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 23 epochs and continued to make progress until the 23rd epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "* *Does BN produce a better model?* Yes! The final model is also much better, with 72.3% validation accuracy instead of 55.0%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
    "* *How does BN affect training speed?* Although the model converged much faster, each epoch took about 15s instead of 10s, because of the extra computations required by the BN layers. But overall the training time (wall time) to reach the best model was shortened by about 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d.\n",
    "*Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haide\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.2737 - loss: 2.0471 - val_accuracy: 0.3710 - val_loss: 1.7810\n",
      "Epoch 2/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.3868 - loss: 1.7248 - val_accuracy: 0.3978 - val_loss: 1.6923\n",
      "Epoch 3/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.4250 - loss: 1.6230 - val_accuracy: 0.4368 - val_loss: 1.6323\n",
      "Epoch 4/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.4561 - loss: 1.5469 - val_accuracy: 0.4332 - val_loss: 1.6292\n",
      "Epoch 5/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.4751 - loss: 1.4952 - val_accuracy: 0.4582 - val_loss: 1.5861\n",
      "Epoch 6/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.4987 - loss: 1.4422 - val_accuracy: 0.4656 - val_loss: 1.5717\n",
      "Epoch 7/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5222 - loss: 1.3880 - val_accuracy: 0.4690 - val_loss: 1.5576\n",
      "Epoch 8/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.5289 - loss: 1.3573 - val_accuracy: 0.4800 - val_loss: 1.5490\n",
      "Epoch 9/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 11ms/step - accuracy: 0.5479 - loss: 1.3161 - val_accuracy: 0.4886 - val_loss: 1.5228\n",
      "Epoch 10/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.5630 - loss: 1.2841 - val_accuracy: 0.4762 - val_loss: 1.5565\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.5685 - loss: 1.2483 - val_accuracy: 0.4800 - val_loss: 1.5957\n",
      "Epoch 12/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5801 - loss: 1.2289 - val_accuracy: 0.4750 - val_loss: 1.5824\n",
      "Epoch 13/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5868 - loss: 1.2067 - val_accuracy: 0.4932 - val_loss: 1.5363\n",
      "Epoch 14/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.5958 - loss: 1.1793 - val_accuracy: 0.4918 - val_loss: 1.5773\n",
      "Epoch 15/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6057 - loss: 1.1552 - val_accuracy: 0.4906 - val_loss: 1.6135\n",
      "Epoch 16/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6041 - loss: 1.1521 - val_accuracy: 0.4886 - val_loss: 1.6264\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6201 - loss: 1.1183 - val_accuracy: 0.4890 - val_loss: 1.6326\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6240 - loss: 1.1076 - val_accuracy: 0.4808 - val_loss: 1.6300\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6292 - loss: 1.0913 - val_accuracy: 0.4860 - val_loss: 1.6308\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.6360 - loss: 1.0730 - val_accuracy: 0.4848 - val_loss: 1.6444\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.6471 - loss: 1.0394 - val_accuracy: 0.4876 - val_loss: 1.6639\n",
      "Epoch 22/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.6521 - loss: 1.0303 - val_accuracy: 0.4928 - val_loss: 1.6704\n",
      "Epoch 23/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6585 - loss: 1.0107 - val_accuracy: 0.4924 - val_loss: 1.6799\n",
      "Epoch 24/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6680 - loss: 0.9898 - val_accuracy: 0.4912 - val_loss: 1.6940\n",
      "Epoch 25/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6698 - loss: 0.9893 - val_accuracy: 0.4914 - val_loss: 1.7157\n",
      "Epoch 26/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6715 - loss: 23.7032 - val_accuracy: 0.4142 - val_loss: 1.7280\n",
      "Epoch 27/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5346 - loss: 1.3545 - val_accuracy: 0.4800 - val_loss: 1.5740\n",
      "Epoch 28/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5977 - loss: 1.1715 - val_accuracy: 0.4840 - val_loss: 1.5986\n",
      "Epoch 29/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.6187 - loss: 1.1088 - val_accuracy: 0.4900 - val_loss: 1.6282\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4829 - loss: 1.5222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5228419303894043, 0.4885999858379364]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                    kernel_initializer=\"lecun_normal\",\n",
    "                                    activation=\"selu\"))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=20, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoints_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_cifar10_selu_model.keras\" ,save_best_only=True\n",
    ")\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_selu_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb,model_checkpoints_cb,tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled,y_train,epochs=100,\n",
    "          validation_data=(X_valid_scaled,y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.evaluate(X_valid_scaled,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model reached the first model's validation loss in just 8 epochs. After 14 epochs, it reached its lowest validation loss, with about 48.3% accuracy, which is better than the original model (46.7%), but not quite as good as the model using batch normalization (50.7%). Each epoch took only 9 seconds. So it's the fastest model to train so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e.\n",
    "*Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haide\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.2723 - loss: 2.0723 - val_accuracy: 0.4022 - val_loss: 1.7003\n",
      "Epoch 2/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.3997 - loss: 1.6956 - val_accuracy: 0.4276 - val_loss: 1.6504\n",
      "Epoch 3/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.4383 - loss: 1.5942 - val_accuracy: 0.4484 - val_loss: 1.5880\n",
      "Epoch 4/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.4690 - loss: 1.5214 - val_accuracy: 0.4702 - val_loss: 1.5933\n",
      "Epoch 5/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.4893 - loss: 1.4689 - val_accuracy: 0.4642 - val_loss: 1.5699\n",
      "Epoch 6/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.5090 - loss: 1.4144 - val_accuracy: 0.4654 - val_loss: 1.6174\n",
      "Epoch 7/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - accuracy: 0.5223 - loss: 1.3639 - val_accuracy: 0.4848 - val_loss: 1.5944\n",
      "Epoch 8/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5468 - loss: 1.3216 - val_accuracy: 0.4858 - val_loss: 1.6545\n",
      "Epoch 9/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5558 - loss: 1.2822 - val_accuracy: 0.4880 - val_loss: 1.6028\n",
      "Epoch 10/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5665 - loss: 1.2496 - val_accuracy: 0.4892 - val_loss: 1.6544\n",
      "Epoch 11/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5783 - loss: 1.2221 - val_accuracy: 0.4994 - val_loss: 1.6087\n",
      "Epoch 12/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.5930 - loss: 1.1873 - val_accuracy: 0.4912 - val_loss: 1.6747\n",
      "Epoch 13/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6017 - loss: 1.1640 - val_accuracy: 0.4934 - val_loss: 1.6826\n",
      "Epoch 14/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6095 - loss: 1.1374 - val_accuracy: 0.4920 - val_loss: 1.7147\n",
      "Epoch 15/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6154 - loss: 1.1205 - val_accuracy: 0.4968 - val_loss: 1.7326\n",
      "Epoch 16/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6284 - loss: 1.0904 - val_accuracy: 0.4882 - val_loss: 1.7297\n",
      "Epoch 17/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6345 - loss: 1.0720 - val_accuracy: 0.4936 - val_loss: 1.7793\n",
      "Epoch 18/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6405 - loss: 1.0546 - val_accuracy: 0.4896 - val_loss: 1.7474\n",
      "Epoch 19/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6497 - loss: 1.0348 - val_accuracy: 0.5030 - val_loss: 1.7244\n",
      "Epoch 20/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6527 - loss: 1.0200 - val_accuracy: 0.5040 - val_loss: 1.7489\n",
      "Epoch 21/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6660 - loss: 0.9924 - val_accuracy: 0.5018 - val_loss: 1.7300\n",
      "Epoch 22/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6742 - loss: 0.9701 - val_accuracy: 0.5044 - val_loss: 1.8024\n",
      "Epoch 23/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6693 - loss: 0.9803 - val_accuracy: 0.4940 - val_loss: 1.8851\n",
      "Epoch 24/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6807 - loss: 0.9453 - val_accuracy: 0.5002 - val_loss: 1.8854\n",
      "Epoch 25/100\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.6921 - loss: 0.9325 - val_accuracy: 0.5076 - val_loss: 1.8514\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4636 - loss: 1.5791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.569853663444519, 0.4641999900341034]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.layers\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                    kernel_initializer=\"lecun_normal\",\n",
    "                                    activation=\"selu\"))\n",
    "    \n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer = optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=20, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_cifar10_alpha_dropout_model.keras\",save_best_only=True\n",
    ")\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_alpha_dropout_{run_index:03d}\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb,model_checkpoint_cb,tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled,y_train,epochs=100,\n",
    "          validation_data=(X_valid_scaled,y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.evaluate(X_valid_scaled,y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reaches 48.1% accuracy on the validation set. That's worse than without dropout (50.3%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the MCAlphaDropout class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self,inputs):\n",
    "        return super().call(inputs,training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with MCAlphaDropout dropout layers instead of AlphaDropout layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = tf.keras.Sequential([\n",
    "    (\n",
    "        MCAlphaDropout(layer.rate)\n",
    "        if isinstance(layer,keras.layers.AlphaDropout)\n",
    "        else layer\n",
    "    )\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mc_dropout_predict_probas(mc_model,X,n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas,axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model,X,n_samples=10):\n",
    "    Y_probas =mc_dropout_predict_probas(mc_model,X,n_samples)\n",
    "    return Y_probas.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4642"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model,X_valid_scaled)\n",
    "accuracy = (y_pred == y_valid[:,0]).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> f.\n",
    "Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haide\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                    kernel_initializer=\"lecun_normal\",\n",
    "                                    activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = tf.keras.backend\n",
    "\n",
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.sum_of_epoch_losses = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        mean_epoch_loss = logs[\"loss\"]  # the epoch's mean loss so far \n",
    "        new_sum_of_epoch_losses = mean_epoch_loss * (batch + 1)\n",
    "        batch_loss = new_sum_of_epoch_losses - self.sum_of_epoch_losses\n",
    "        self.sum_of_epoch_losses = new_sum_of_epoch_losses\n",
    "        lr = self.model.optimizer.learning_rate.numpy()\n",
    "        self.rates.append(lr)\n",
    "        self.losses.append(batch_loss)\n",
    "        self.model.optimizer.learning_rate = lr * self.factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,\n",
    "                       max_rate=1):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    model.optimizer.learning_rate = min_rate\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    model.optimizer.learning_rate = init_lr\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses, \"b\")\n",
    "    plt.gca().set_xscale('log')\n",
    "    max_loss = losses[0] + min(losses)\n",
    "    plt.hlines(min(losses), min(rates), max(rates), color=\"k\")\n",
    "    plt.axis([min(rates), max(rates), 0, max_loss])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.1517 - loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size  = 128\n",
    "rates,losses = find_learning_rate(model,X_train_scaled,y_train,\n",
    "                                  epochs=1,batch_size=batch_size)\n",
    "\n",
    "plot_lr_vs_loss(rates,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haide\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(tf.keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=2e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,iterations,max_lr = 1e-3,start_lr=None,\n",
    "                 last_iterations=None,last_lr=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_lr = max_lr\n",
    "        self.start_lr = start_lr or max_lr / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_lr = last_lr or self.start_lr / 1000\n",
    "        self.iteration = 0\n",
    "\n",
    "    def _interpolate(self,iter1,iter2,lr1,lr2):\n",
    "        return (lr2-lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
    "    \n",
    "    def on_batch_begin(self,batch,logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            lr = self._interpolate(0,self.half_iteration,self.start_lr,\n",
    "                                   self.max_lr)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                   self.max_lr,self.start_lr)\n",
    "        else:\n",
    "            lr = self._interpolate(2* self.half_iteration,self.iterations,\n",
    "                                   self.max_lr,self.last_lr)\n",
    "        self.iteration += 1\n",
    "        self.model.optimizer.learning_rate = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.2318 - loss: 2.2404 - val_accuracy: 0.3804 - val_loss: 1.7564\n",
      "Epoch 2/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.3631 - loss: 1.8062 - val_accuracy: 0.4162 - val_loss: 1.6708\n",
      "Epoch 3/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.4213 - loss: 1.6440 - val_accuracy: 0.4192 - val_loss: 1.6578\n",
      "Epoch 4/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.4484 - loss: 1.5589 - val_accuracy: 0.4288 - val_loss: 1.6729\n",
      "Epoch 5/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.4715 - loss: 1.4983 - val_accuracy: 0.4410 - val_loss: 1.6715\n",
      "Epoch 6/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.4890 - loss: 1.4509 - val_accuracy: 0.4450 - val_loss: 1.6729\n",
      "Epoch 7/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.4974 - loss: 1.4154 - val_accuracy: 0.4614 - val_loss: 1.6226\n",
      "Epoch 8/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5218 - loss: 1.3525 - val_accuracy: 0.4660 - val_loss: 1.6393\n",
      "Epoch 9/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5469 - loss: 1.2742 - val_accuracy: 0.4868 - val_loss: 1.6090\n",
      "Epoch 10/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5748 - loss: 1.2008 - val_accuracy: 0.4988 - val_loss: 1.6064\n",
      "Epoch 11/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5974 - loss: 1.1316 - val_accuracy: 0.4940 - val_loss: 1.6191\n",
      "Epoch 12/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6211 - loss: 1.0667 - val_accuracy: 0.4992 - val_loss: 1.6436\n",
      "Epoch 13/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6453 - loss: 1.0028 - val_accuracy: 0.5078 - val_loss: 1.6317\n",
      "Epoch 14/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.6466 - loss: 0.9917 - val_accuracy: 0.4868 - val_loss: 1.7011\n",
      "Epoch 15/15\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6199 - loss: 1.0690 - val_accuracy: 0.5126 - val_loss: 1.5919\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "n_iterations = math.ceil(len(X_train_scaled) / batch_size) * n_epochs\n",
    "onecycle = OneCycleScheduler(n_iterations, max_lr=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 50.7% to 52.0%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
